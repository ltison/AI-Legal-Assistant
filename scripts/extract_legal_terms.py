"""
Skript na extrahovanie prÃ¡vnych pojmov z textov zÃ¡konov pomocÃ­ OpenAI API
"""

import os
import json
import sqlite3
import re
from typing import List, Dict, Tuple
from pathlib import Path
import sys

# Pridaj project root do Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from openai import OpenAI
    from dotenv import load_dotenv
    load_dotenv()
    OPENAI_AVAILABLE = True
except ImportError:
    print("âŒ OpenAI kniÅ¾nica nie je dostupnÃ¡")
    OPENAI_AVAILABLE = False


class LegalTermExtractor:
    """Extrahuje prÃ¡vne pojmy z textov zÃ¡konov pomocÃ­ OpenAI API"""
    
    def __init__(self, data_dir: str = "data/law_texts", db_path: str = "data/legal_terms.db"):
        self.data_dir = Path(data_dir)
        self.db_path = Path(db_path)
        
        # Vytvor adresÃ¡r pre databÃ¡zu ak neexistuje
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Inicializuj OpenAI klienta
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            print("âœ… OpenAI klient inicializovanÃ½")
        else:
            self.client = None
            print("âŒ OpenAI API key nie je nastavenÃ½")
        
        # Inicializuj databÃ¡zu
        self.init_database()
    
    def init_database(self):
        """VytvorÃ­ SQL databÃ¡zu pre prÃ¡vne pojmy"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Vytvor tabuÄ¾ku pre prÃ¡vne pojmy
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS legal_terms (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    term TEXT NOT NULL,
                    definition TEXT NOT NULL,
                    law_id TEXT NOT NULL,
                    paragraph TEXT,
                    context TEXT,
                    confidence REAL DEFAULT 0.0,
                    category TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(term, law_id, paragraph)
                )
            ''')
            
            # Index pre rÃ½chlejÅ¡ie vyhÄ¾adÃ¡vanie
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_term ON legal_terms(term)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_law_id ON legal_terms(law_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON legal_terms(category)')
            
            conn.commit()
            conn.close()
            print("âœ… SQL databÃ¡za inicializovanÃ¡")
            
        except Exception as e:
            print(f"âŒ Chyba pri inicializÃ¡cii databÃ¡zy: {e}")
    
    def load_metadata(self) -> Dict:
        """NaÄÃ­ta metadÃ¡ta sÃºborov"""
        metadata_path = self.data_dir / "files_metadata.json"
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ Chyba pri naÄÃ­tanÃ­ metadÃ¡t: {e}")
            return {"files": []}
    
    def chunk_text_for_ai(self, text: str, chunk_size: int = 2000) -> List[str]:
        """RozdelÃ­ text na chunky vhodnÃ© pre OpenAI API"""
        # RozdeÄ¾ podÄ¾a paragrafov
        paragraphs = re.split(r'(Â§\s*\d+[a-z]*)', text)
        
        chunks = []
        current_chunk = ""
        current_paragraph = ""
        
        for i, part in enumerate(paragraphs):
            # Je to paragraf?
            if re.match(r'Â§\s*\d+[a-z]*', part.strip()):
                current_paragraph = part.strip()
                # Ak by chunk bol prÃ­liÅ¡ veÄ¾kÃ½, uloÅ¾ ho
                if len(current_chunk) > chunk_size:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = current_paragraph + "\n"
                else:
                    current_chunk += current_paragraph + "\n"
            else:
                # Je to obsah paragrafu
                if len(current_chunk + part) > chunk_size and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = current_paragraph + "\n" + part
                else:
                    current_chunk += part
        
        # Pridaj poslednÃ½ chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def extract_terms_with_ai(self, text_chunk: str, law_info: Dict) -> List[Dict]:
        """Extrahuje prÃ¡vne pojmy z textu pomocÃ­ OpenAI"""
        if not self.client:
            return []
        
        try:
            # Prompt pre GPT-4o-mini
            prompt = f"""
Analyzuj tento text zo slovenskÃ©ho zÃ¡kona a extrahuj vÅ¡etky prÃ¡vne pojmy a ich definÃ­cie.

ZÃKON: {law_info['title']} ({law_info['law_id']})
KATEGÃ“RIA: {law_info.get('category', 'neznÃ¡ma')}

TEXT:
{text_chunk}

INÅ TRUKCIE:
1. HÄ¾adaj explicitnÃ© definÃ­cie (obsahujÃºce "rozumie sa", "znamenÃ¡", "je to", "je definovanÃ© ako")
2. HÄ¾adaj implicitnÃ© definÃ­cie z kontextu paragrafov
3. Ignoruj Äisto procedurÃ¡lne ustanovenia bez definÃ­ciÃ­
4. Pre kaÅ¾dÃ½ pojem uveÄ:
   - term: presnÃ½ nÃ¡zov pojmu (krÃ¡tko)
   - definition: definÃ­cia (1-3 vety)
   - paragraph: ÄÃ­slo paragrafu (ak je zrejmÃ©)
   - confidence: hodnotenie 0.0-1.0 ako si istÃ½
   - category: typ pojmu (subjekt, ÄinnosÅ¥, dokument, lehota, povinnosÅ¥, prÃ¡vo, inÃ©)

VrÃ¡Å¥ JSON array vo formÃ¡te:
[
  {{
    "term": "nÃ¡zov pojmu",
    "definition": "definÃ­cia pojmu",
    "paragraph": "Â§ X", 
    "confidence": 0.8,
    "category": "kategÃ³ria"
  }}
]

Ak nenÃ¡jdeÅ¡ Å¾iadne definÃ­cie, vrÃ¡Å¥ prÃ¡zdny array [].
"""

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Si expert na slovenskÃ© prÃ¡vo. ExtraktujeÅ¡ presne a sprÃ¡vne prÃ¡vne pojmy a ich definÃ­cie zo zÃ¡konov."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            # Parsuj odpoveÄ
            content = response.choices[0].message.content.strip()
            
            # OdstrÃ¡Åˆ markdown bloky ak sÃº
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            
            # Parsuj JSON
            terms = json.loads(content)
            
            # Validuj a doplÅˆ dÃ¡ta
            validated_terms = []
            for term_data in terms:
                if isinstance(term_data, dict) and "term" in term_data and "definition" in term_data:
                    # DoplÅˆ chÃ½bajÃºce polia
                    term_data["law_id"] = law_info["law_id"]
                    term_data["context"] = text_chunk[:200] + "..." if len(text_chunk) > 200 else text_chunk
                    
                    # Validuj confidence
                    if "confidence" not in term_data:
                        term_data["confidence"] = 0.5
                    
                    validated_terms.append(term_data)
            
            return validated_terms
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON parse error: {e}")
            print(f"OdpoveÄ: {content[:200]}...")
            return []
        except Exception as e:
            print(f"âŒ Chyba pri volanÃ­ OpenAI API: {e}")
            return []
    
    def save_terms_to_db(self, terms: List[Dict]) -> int:
        """UloÅ¾Ã­ extrahovanÃ© pojmy do databÃ¡zy"""
        if not terms:
            return 0
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            saved_count = 0
            for term_data in terms:
                try:
                    cursor.execute('''
                        INSERT OR REPLACE INTO legal_terms 
                        (term, definition, law_id, paragraph, context, confidence, category)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        term_data.get("term", ""),
                        term_data.get("definition", ""),
                        term_data.get("law_id", ""),
                        term_data.get("paragraph", ""),
                        term_data.get("context", ""),
                        float(term_data.get("confidence", 0.0)),
                        term_data.get("category", "inÃ©")
                    ))
                    saved_count += 1
                except Exception as e:
                    print(f"âš ï¸ Chyba pri ukladanÃ­ pojmu {term_data.get('term', 'N/A')}: {e}")
            
            conn.commit()
            conn.close()
            
            return saved_count
            
        except Exception as e:
            print(f"âŒ Chyba pri ukladanÃ­ do databÃ¡zy: {e}")
            return 0
    
    def process_file(self, file_info: Dict) -> int:
        """Spracuje jeden sÃºbor a extrahuje z neho pojmy"""
        filepath = self.data_dir / file_info["filename"]
        
        try:
            print(f"ğŸ“– SpracovÃ¡vam: {file_info['title']} ({file_info['law_id']})")
            
            # NaÄÃ­taj obsah
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # RozdeÄ¾ na chunky
            chunks = self.chunk_text_for_ai(content)
            print(f"   ğŸ“„ VytvorenÃ½ch {len(chunks)} chunkov na analÃ½zu")
            
            all_terms = []
            for i, chunk in enumerate(chunks):
                print(f"   ğŸ¤– Analyzujem chunk {i+1}/{len(chunks)}...")
                
                # Extrahuj pojmy pomocÃ­ AI
                terms = self.extract_terms_with_ai(chunk, file_info)
                all_terms.extend(terms)
                
                if terms:
                    print(f"   âœ… NÃ¡jdenÃ½ch {len(terms)} pojmov v chunku {i+1}")
            
            # UloÅ¾ do databÃ¡zy
            saved_count = self.save_terms_to_db(all_terms)
            
            print(f"   ğŸ’¾ UloÅ¾enÃ½ch {saved_count} pojmov z {file_info['law_id']}")
            return saved_count
            
        except Exception as e:
            print(f"âŒ Chyba pri spracovanÃ­ {file_info['filename']}: {e}")
            return 0
    
    def extract_all_terms(self) -> int:
        """Extrahuje pojmy zo vÅ¡etkÃ½ch sÃºborov"""
        if not self.client:
            print("âŒ OpenAI API nie je dostupnÃ©")
            return 0
        
        metadata = self.load_metadata()
        files = metadata.get("files", [])
        
        print(f"ğŸš€ SpÃºÅ¡Å¥am extrahovanie pojmov z {len(files)} sÃºborov")
        print("=" * 60)
        
        total_terms = 0
        for file_info in files:
            terms_count = self.process_file(file_info)
            total_terms += terms_count
        
        print("=" * 60)
        print(f"ğŸ‰ Celkovo extrahovanÃ½ch {total_terms} prÃ¡vnych pojmov!")
        
        # Zobraz Å¡tatistiky
        self.show_statistics()
        
        return total_terms
    
    def show_statistics(self):
        """ZobrazÃ­ Å¡tatistiky databÃ¡zy"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # CelkovÃ½ poÄet pojmov
            cursor.execute("SELECT COUNT(*) FROM legal_terms")
            total_count = cursor.fetchone()[0]
            
            # PoÄet podÄ¾a zÃ¡konov
            cursor.execute("""
                SELECT law_id, COUNT(*) as count 
                FROM legal_terms 
                GROUP BY law_id 
                ORDER BY count DESC
            """)
            by_law = cursor.fetchall()
            
            # PoÄet podÄ¾a kategÃ³riÃ­
            cursor.execute("""
                SELECT category, COUNT(*) as count 
                FROM legal_terms 
                GROUP BY category 
                ORDER BY count DESC
            """)
            by_category = cursor.fetchall()
            
            print(f"\nğŸ“Š Å tatistiky databÃ¡zy prÃ¡vnych pojmov:")
            print(f"ğŸ“ CelkovÃ½ poÄet pojmov: {total_count}")
            
            print(f"\nâš–ï¸ Rozdelenie podÄ¾a zÃ¡konov:")
            for law_id, count in by_law:
                print(f"   {law_id}: {count} pojmov")
            
            print(f"\nğŸ·ï¸ Rozdelenie podÄ¾a kategÃ³riÃ­:")
            for category, count in by_category:
                print(f"   {category}: {count} pojmov")
            
            conn.close()
            
        except Exception as e:
            print(f"âš ï¸ Chyba pri zÃ­skavanÃ­ Å¡tatistÃ­k: {e}")
    
    def test_search(self, search_term: str = "spoloÄnosÅ¥"):
        """Test vyhÄ¾adÃ¡vania v databÃ¡ze"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            print(f"\nğŸ” Test vyhÄ¾adÃ¡vania: '{search_term}'")
            
            # Fuzzy search
            cursor.execute("""
                SELECT term, definition, law_id, paragraph, confidence, category
                FROM legal_terms 
                WHERE term LIKE ? OR definition LIKE ?
                ORDER BY confidence DESC
                LIMIT 5
            """, (f"%{search_term}%", f"%{search_term}%"))
            
            results = cursor.fetchall()
            
            if results:
                for i, (term, definition, law_id, paragraph, confidence, category) in enumerate(results, 1):
                    print(f"\n{i}. {term} ({category})")
                    print(f"   ğŸ“ {law_id} {paragraph if paragraph else ''}")
                    print(f"   ğŸ“ {definition[:150]}...")
                    print(f"   ğŸ¯ Confidence: {confidence:.2f}")
            else:
                print("âŒ Å½iadne vÃ½sledky")
            
            conn.close()
            
        except Exception as e:
            print(f"âŒ Chyba pri teste vyhÄ¾adÃ¡vania: {e}")


def main():
    """HlavnÃ¡ funkcia"""
    print("ğŸš€ Extrahovanie prÃ¡vnych pojmov z textov zÃ¡konov")
    print("=" * 50)
    
    # Vytvor extraktor
    extractor = LegalTermExtractor()
    
    # Extrahuj pojmy
    count = extractor.extract_all_terms()
    
    if count > 0:
        # Testuj vyhÄ¾adÃ¡vanie
        extractor.test_search("spoloÄnosÅ¥")
        extractor.test_search("vlastnÃ­ctvo")
        
    else:
        print("\nâŒ Å½iadne pojmy neboli extrahovanÃ©")


if __name__ == "__main__":
    main()
