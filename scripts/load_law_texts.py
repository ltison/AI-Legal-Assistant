"""
Skript na naÄÃ­tanie a chunkovanie prÃ¡vnych textov do ChromaDB
"""

import os
import re
import sys
import json
from typing import List, Dict, Tuple
from pathlib import Path

# Pridaj project root do Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    import chromadb
    from chromadb.config import Settings
    from sentence_transformers import SentenceTransformer
    CHROMADB_AVAILABLE = True
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    print(f"âŒ ChÃ½bajÃºce kniÅ¾nice: {e}")
    print("ğŸ’¡ Spustite: pip install chromadb sentence-transformers")
    CHROMADB_AVAILABLE = False
    SENTENCE_TRANSFORMERS_AVAILABLE = False


class LegalTextLoader:
    """NaÄÃ­tava a spracovÃ¡va prÃ¡vne texty do ChromaDB s optimÃ¡lnym chunkovanÃ­m"""
    
    def __init__(self, data_dir: str = "data/law_texts", db_path: str = "data/vector_db"):
        self.data_dir = Path(data_dir)
        self.db_path = Path(db_path)
        
        # Nastavenia pre chunkovanie - optimalizovanÃ© pre zachovanie kontextu
        self.chunk_size = 2000  # VÃ¤ÄÅ¡ie chunky pre lepÅ¡Ã­ kontext (â‰ˆ500 tokenov)
        self.chunk_overlap = 400  # VeÄ¾kÃ½ prekryv pre zachovanie kontextu
        self.min_chunk_size = 500  # MinimÃ¡lna veÄ¾kosÅ¥ chunku pre zachovanie vÃ½znamu
        
        if CHROMADB_AVAILABLE:
            try:
                # InicializÃ¡cia ChromaDB s lepÅ¡Ã­m embedding modelom
                self.client = chromadb.PersistentClient(path=str(self.db_path))
                
                # Pokus o slovenskÃ½/multilingual model
                if SENTENCE_TRANSFORMERS_AVAILABLE:
                    try:
                        # Multilingual model s dobrou podporou slovenÄiny
                        embedding_model = "paraphrase-multilingual-MiniLM-L12-v2"
                        print(f"ğŸ¤– NaÄÃ­tavam embedding model: {embedding_model}")
                        
                        # Vytvor embedding funkciu s normalizÃ¡ciou pre konzistenciu
                        class MultilingualEmbeddingFunction:
                            def __init__(self, model_name):
                                from sentence_transformers import SentenceTransformer
                                self.model = SentenceTransformer(model_name)
                                self.model_name = model_name  # Pridaj nÃ¡zov modelu
                                self.name = f"multilingual-{model_name}"  # ChromaDB name atribÃºt
                            
                            def __call__(self, input):
                                import numpy as np
                                
                                # ZÃ­skaj embeddings
                                embeddings = self.model.encode(input)
                                
                                # Normalizuj pre konzistenciu
                                if len(embeddings.shape) == 1:
                                    # Jeden vektor
                                    norm = np.linalg.norm(embeddings)
                                    if norm > 0:
                                        embeddings = embeddings / norm
                                else:
                                    # Viac vektorov
                                    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
                                    embeddings = embeddings / np.maximum(norms, 1e-8)
                                
                                return embeddings.tolist()
                        
                        embedding_function = MultilingualEmbeddingFunction(embedding_model)
                        print("âœ… Multilingual embedding model naÄÃ­tanÃ½")
                        
                    except Exception as e:
                        print(f"âš ï¸ Chyba pri naÄÃ­tanÃ­ embedding modelu: {e}")
                        embedding_function = None
                else:
                    embedding_function = None
                
                # VymaÅ¾ existujÃºcu collection a vytvor novÃº
                try:
                    collections = self.client.list_collections()
                    for collection in collections:
                        if collection.name == "legal_documents":
                            self.client.delete_collection("legal_documents")
                            print("ğŸ—‘ï¸ VymazanÃ¡ starÃ¡ collection")
                except:
                    pass
                
                # Vytvor novÃº collection s lepÅ¡Ã­m embedding modelom
                if embedding_function:
                    self.collection = self.client.create_collection(
                        name="legal_documents",
                        embedding_function=embedding_function,
                        metadata={"description": "SlovenskÃ© a ÄeskÃ© prÃ¡vne predpisy - optimÃ¡lne embeddingy"}
                    )
                else:
                    self.collection = self.client.create_collection(
                        name="legal_documents",
                        metadata={"description": "SlovenskÃ© a ÄeskÃ© prÃ¡vne predpisy"}
                    )
                
                print("âœ… ChromaDB collection vytvorenÃ¡ s novÃ½mi nastaveniami")
                
            except Exception as e:
                print(f"âŒ Chyba pri inicializÃ¡cii ChromaDB: {e}")
                self.client = None
                self.collection = None
        else:
            self.client = None
            self.collection = None
    
    def load_metadata(self) -> Dict[str, Dict]:
        """
        NaÄÃ­ta metadÃ¡ta sÃºborov z JSON sÃºboru
        """
        metadata_path = self.data_dir / "files_metadata.json"
        if not metadata_path.exists():
            print(f"âŒ Metadata sÃºbor {metadata_path} neexistuje")
            return {}
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Vytvor mapping filename -> metadata
            metadata_map = {}
            for file_info in data.get('files', []):
                filename = file_info['filename']
                metadata_map[filename] = file_info
            
            print(f"âœ… NaÄÃ­tanÃ© metadÃ¡ta pre {len(metadata_map)} sÃºborov")
            return metadata_map
            
        except Exception as e:
            print(f"âŒ Chyba pri naÄÃ­tavanÃ­ metadÃ¡t: {e}")
            return {}
    
    def smart_chunk_text(self, text: str, law_info: Dict) -> List[Dict]:
        """
        InteligentnÃ© chunkovanie s dÃ´razom na zachovanie kontextu
        - MinimÃ¡lne 500 tokenov (~2000 znakov) na chunk
        - VeÄ¾kÃ½ prekryv pre zachovanie kontextu
        - ReÅ¡pektovanie Å¡truktÃºry paragrafov ale spÃ¡janie malÃ½ch
        """
        chunks = []
        
        # Najprv skÃºs rozdeliÅ¥ podÄ¾a paragrafov ale spÃ¡jaj malÃ©
        paragraph_chunks = self._chunk_by_paragraphs_contextual(text, law_info)
        
        if paragraph_chunks:
            chunks = paragraph_chunks
        else:
            # Ak nie sÃº paragrafy, pouÅ¾ij kontextuÃ¡lne oknÃ¡
            chunks = self._contextual_window_chunk(text, law_info)
        
        # Filtrovanie prÃ­liÅ¡ malÃ½ch chunkov
        chunks = self._merge_small_chunks(chunks, law_info)
        
        return chunks
    
    def _chunk_by_paragraphs_contextual(self, text: str, law_info: Dict) -> List[Dict]:
        """RozdelÃ­ text podÄ¾a paragrafov ale zachovÃ¡ kontext spÃ¡janÃ­m malÃ½ch"""
        chunks = []
        
        # Rozdelenie podÄ¾a paragrafov (Â§ X)
        paragraph_pattern = r'(Â§\s*\d+[a-z]*(?:\s*[a-z]\))?)'
        parts = re.split(paragraph_pattern, text)
        
        current_paragraphs = []
        current_text = ""
        chunk_counter = 0
        
        for i, part in enumerate(parts):
            if re.match(paragraph_pattern, part.strip()):
                # NovÃ½ paragraf
                new_paragraph = part.strip()
                
                # Ak aktuÃ¡lny chunk je dosÅ¥ veÄ¾kÃ½, uloÅ¾ ho
                if current_text and len(current_text) >= self.min_chunk_size:
                    chunk_counter += 1
                    chunks.append(self._create_contextual_chunk(
                        paragraphs=current_paragraphs,
                        text=current_text.strip(),
                        law_info=law_info,
                        chunk_num=chunk_counter
                    ))
                    
                    # Zachovaj prekryv - ponechaj poslednÃ½ paragraf
                    if current_paragraphs:
                        last_para = current_paragraphs[-1]
                        last_para_text = self._get_paragraph_text(text, last_para)
                        current_paragraphs = [last_para]
                        current_text = f"{last_para}\n\n{last_para_text}\n\n"
                    else:
                        current_paragraphs = []
                        current_text = ""
                
                current_paragraphs.append(new_paragraph)
                
            else:
                # Text patriaci k paragrafu
                current_text += part
        
        # UloÅ¾ poslednÃ½ chunk
        if current_paragraphs and current_text.strip():
            chunk_counter += 1
            chunks.append(self._create_contextual_chunk(
                paragraphs=current_paragraphs,
                text=current_text.strip(),
                law_info=law_info,
                chunk_num=chunk_counter
            ))
        
        return chunks
    
    def _contextual_window_chunk(self, text: str, law_info: Dict) -> List[Dict]:
        """KontextuÃ¡lne oknÃ¡ s veÄ¾kÃ½m prekryvom"""
        chunks = []
        
        # RozdeÄ¾ na vety
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        current_chunk = ""
        chunk_num = 1
        i = 0
        
        while i < len(sentences):
            # PridÃ¡vaj vety pokÃ½m nedosiahneÅ¡ cieÄ¾ovÃº veÄ¾kosÅ¥
            while i < len(sentences) and len(current_chunk) < self.chunk_size:
                current_chunk += sentences[i] + ". "
                i += 1
            
            # Ak je chunk prÃ­liÅ¡ malÃ½, pridaj eÅ¡te vety
            while i < len(sentences) and len(current_chunk) < self.min_chunk_size:
                current_chunk += sentences[i] + ". "
                i += 1
            
            if current_chunk.strip():
                chunks.append(self._create_contextual_chunk(
                    paragraphs=[f"ÄŒasÅ¥ {chunk_num}"],
                    text=current_chunk.strip(),
                    law_info=law_info,
                    chunk_num=chunk_num
                ))
                
                # VeÄ¾kÃ½ prekryv - vrÃ¡Å¥ sa o 1/3 textu
                overlap_chars = len(current_chunk) // 3
                overlap_text = current_chunk[-overlap_chars:]
                overlap_sentences = len(overlap_text.split('.'))
                
                i = max(0, i - overlap_sentences)
                current_chunk = ""
                chunk_num += 1
        
        return chunks
    
    def _merge_small_chunks(self, chunks: List[Dict], law_info: Dict) -> List[Dict]:
        """SpÃ¡ja prÃ­liÅ¡ malÃ© chunky so susednÃ½mi"""
        if not chunks:
            return chunks
        
        merged_chunks = []
        i = 0
        
        while i < len(chunks):
            current_chunk = chunks[i]
            
            # Ak je chunk prÃ­liÅ¡ malÃ½ a nie je poslednÃ½
            if (len(current_chunk['text']) < self.min_chunk_size and 
                i < len(chunks) - 1):
                
                # Spoj s nasledujÃºcim
                next_chunk = chunks[i + 1]
                
                merged_text = current_chunk['text'] + "\n\n" + next_chunk['text']
                merged_paragraphs = (current_chunk['metadata']['paragraphs'] + 
                                   next_chunk['metadata']['paragraphs'])
                
                merged_chunk = self._create_contextual_chunk(
                    paragraphs=merged_paragraphs,
                    text=merged_text,
                    law_info=law_info,
                    chunk_num=f"{current_chunk['metadata']['chunk_num']}-{next_chunk['metadata']['chunk_num']}"
                )
                
                merged_chunks.append(merged_chunk)
                i += 2  # PreskoÄme oba chunky
            else:
                merged_chunks.append(current_chunk)
                i += 1
        
        return merged_chunks
    
    def _get_paragraph_text(self, full_text: str, paragraph: str) -> str:
        """Extrahuje text patriaci k paragrafu"""
        # NÃ¡jdi pozÃ­ciu paragrafu v texte
        para_pos = full_text.find(paragraph)
        if para_pos == -1:
            return ""
        
        # NÃ¡jdi nasledujÃºci paragraf
        next_para_pattern = r'Â§\s*\d+[a-z]*(?:\s*[a-z]\))?'
        remaining_text = full_text[para_pos + len(paragraph):]
        
        next_match = re.search(next_para_pattern, remaining_text)
        if next_match:
            return remaining_text[:next_match.start()].strip()
        else:
            return remaining_text.strip()
    
    def _create_contextual_chunk(self, paragraphs: List[str], text: str, law_info: Dict, chunk_num) -> Dict:
        """VytvorÃ­ chunk s kontextovÃ½mi metadÃ¡tami"""
        
        # VyÄisti text
        clean_text = re.sub(r'\s+', ' ', text).strip()
        
        # Vytvor unikÃ¡tne ID
        para_ids = "_".join([p.replace('Â§', 'par').replace(' ', '_') for p in paragraphs[:3]])
        chunk_id = f"{law_info['law_id']}_{para_ids}_{chunk_num}"
        chunk_id = re.sub(r'[^\w_-]', '_', chunk_id)[:100]  # Obmedzme dÄºÅ¾ku
        
        # HlavnÃ½ paragraf (prvÃ½)
        main_paragraph = paragraphs[0] if paragraphs else "N/A"
        
        return {
            "id": chunk_id,
            "text": clean_text,
            "metadata": {
                "law_id": law_info["law_id"],
                "title": law_info["title"],
                "category": law_info.get("category", ""),
                "paragraph": main_paragraph,
                "paragraphs": ", ".join(paragraphs),  # Konvertuj na string pre ChromaDB
                "filename": law_info["filename"],
                "type": "legal_text",
                "chunk_num": str(chunk_num),
                "text_length": len(clean_text),
                "chunk_method": "contextual",
                "token_estimate": len(clean_text) // 4  # HrubÃ½ odhad tokenov
            }
        }
    
    def extract_law_title(self, text: str) -> str:
        """Extrahuje nÃ¡zov zÃ¡kona z textu"""
        lines = text.split('\n')[:20]  # Pozri prvÃ½ch 20 riadkov
        
        for line in lines:
            line = line.strip()
            if any(keyword in line.upper() for keyword in ['ZÃKONNÃK', 'ZÃKON', 'VYHLÃÅ KA']):
                if len(line) < 100:  # RozumnÃ¡ dÄºÅ¾ka nÃ¡zvu
                    return line
        
        return "NeurÄenÃ½ prÃ¡vny predpis"
    
    def load_file(self, filepath: Path, metadata: Dict) -> List[Dict]:
        """NaÄÃ­ta a spracuje jeden sÃºbor s kontextovÃ½m chunkovanÃ­m"""
        try:
            print(f"ğŸ“– SpracovÃ¡vam: {filepath.name}")
            
            # NaÄÃ­taj obsah
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # PouÅ¾ij kontextovÃ© chunkovanie
            chunks = self.smart_chunk_text(content, metadata)
            
            # Å tatistiky
            total_chars = sum(len(chunk['text']) for chunk in chunks)
            avg_chunk_size = total_chars // len(chunks) if chunks else 0
            min_chunk = min(len(chunk['text']) for chunk in chunks) if chunks else 0
            max_chunk = max(len(chunk['text']) for chunk in chunks) if chunks else 0
            
            print(f"âœ… VytvorenÃ½ch {len(chunks)} kontextovÃ½ch chunkov pre {metadata['law_id']} - {metadata['title']}")
            print(f"   ğŸ“Š VeÄ¾kosÅ¥ chunkov: min={min_chunk}, avg={avg_chunk_size}, max={max_chunk} znakov")
            
            return chunks
            
        except Exception as e:
            print(f"âŒ Chyba pri spracovanÃ­ {filepath.name}: {e}")
            return []
    
    def clear_collection(self):
        """VymaÅ¾e vÅ¡etky existujÃºce dÃ¡ta z ChromaDB kolekcie"""
        try:
            # ZÃ­skaj poÄet existujÃºcich zÃ¡znamov
            count = self.collection.count()
            if count > 0:
                print(f"ğŸ—‘ï¸ VymazÃ¡vam {count} existujÃºcich zÃ¡znamov...")
                
                # VymaÅ¾ vÅ¡etky dÃ¡ta
                self.collection.delete()
                print("âœ… Kolekcia ÃºspeÅ¡ne vymazanÃ¡")
            else:
                print("â„¹ï¸ Kolekcia je uÅ¾ prÃ¡zdna")
                
        except Exception as e:
            print(f"âš ï¸ Chyba pri vymazÃ¡vanÃ­ kolekcie: {e}")

    def load_all_files(self) -> int:
        """NaÄÃ­ta vÅ¡etky sÃºbory pomocou JSON metadÃ¡t"""
        if not CHROMADB_AVAILABLE or not self.collection:
            print("âŒ ChromaDB nie je dostupnÃ©")
            return 0
        
        # Najprv vymaÅ¾ existujÃºce dÃ¡ta
        self.clear_collection()
        
        # NaÄÃ­taj metadÃ¡ta sÃºborov
        metadata_map = self.load_metadata()
        if not metadata_map:
            print("âŒ Å½iadne metadÃ¡ta sÃºborov")
            return 0
        
        print(f"ğŸ“‹ NÃ¡jdenÃ½ch {len(metadata_map)} sÃºborov na spracovanie")
        
        all_chunks = []
        for filename, metadata in metadata_map.items():
            filepath = self.data_dir / filename
            if filepath.exists():
                chunks = self.load_file(filepath, metadata)
                all_chunks.extend(chunks)
            else:
                print(f"âš ï¸ SÃºbor {filename} neexistuje")
        
        # Nahraj do ChromaDB s lepÅ¡Ã­m batchingom
        if all_chunks:
            print(f"\nğŸ”„ NahrÃ¡vam {len(all_chunks)} kontextovÃ½ch chunkov do ChromaDB...")
            
            # Priprav dÃ¡ta pre ChromaDB
            ids = [chunk["id"] for chunk in all_chunks]
            documents = [chunk["text"] for chunk in all_chunks]
            metadatas = [chunk["metadata"] for chunk in all_chunks]
            
            try:
                # Pridaj novÃ© dÃ¡ta po menÅ¡Ã­ch dÃ¡vkach pre stabilitu
                batch_size = 50  # MenÅ¡ie dÃ¡vky pre lepÅ¡iu stabilitu
                successful_chunks = 0
                
                for i in range(0, len(ids), batch_size):
                    batch_ids = ids[i:i+batch_size]
                    batch_docs = documents[i:i+batch_size] 
                    batch_metas = metadatas[i:i+batch_size]
                    
                    try:
                        self.collection.add(
                            ids=batch_ids,
                            documents=batch_docs,
                            metadatas=batch_metas
                        )
                        successful_chunks += len(batch_ids)
                        print(f"âœ… NahranÃ½ch {successful_chunks}/{len(ids)} chunkov")
                    except Exception as e:
                        print(f"âš ï¸ Chyba pri batch {i//batch_size + 1}: {e}")
                        continue
                
                if successful_chunks > 0:
                    print(f"ğŸ‰ ÃšspeÅ¡ne nahranÃ½ch {successful_chunks} kontextovÃ½ch chunkov!")
                    
                    # Zobraz Å¡tatistiky
                    self.show_statistics()
                    
                    return successful_chunks
                else:
                    print("âŒ Å½iadne chunky neboli ÃºspeÅ¡ne nahranÃ©")
                    return 0
                
            except Exception as e:
                print(f"âŒ KritickÃ¡ chyba pri nahrÃ¡vanÃ­ do ChromaDB: {e}")
                return 0
        
        return 0
    
    def show_statistics(self):
        """ZobrazÃ­ Å¡tatistiky nahranÃ½ch dÃ¡t"""
        if not self.collection:
            return
        
        try:
            count = self.collection.count()
            print(f"\nğŸ“Š Å tatistiky ChromaDB:")
            print(f"ğŸ“ CelkovÃ½ poÄet chunkov: {count}")
            
            # SkÃºs zÃ­skaÅ¥ vzorku dÃ¡t
            sample = self.collection.get(limit=5)
            if sample and sample['metadatas']:
                laws = set()
                paragraphs = 0
                for meta in sample['metadatas']:
                    if 'law_id' in meta:
                        laws.add(meta['law_id'])
                    if 'paragraph' in meta and meta['paragraph'].startswith('Â§'):
                        paragraphs += 1
                
                print(f"âš–ï¸ ZÃ¡kony v databÃ¡ze: {', '.join(sorted(laws))}")
                print(f"ğŸ“– Paragrafy vo vzorke: {paragraphs}/{len(sample['metadatas'])}")
                
        except Exception as e:
            print(f"âš ï¸ Chyba pri zÃ­skavanÃ­ Å¡tatistÃ­k: {e}")
    
    def test_search(self, query: str = "vlastnÃ­ctvo"):
        """Testuje vyhÄ¾adÃ¡vanie v nahranÃ½ch dÃ¡tach"""
        if not self.collection:
            print("âŒ DatabÃ¡za nie je dostupnÃ¡")
            return
        
        try:
            print(f"\nğŸ” Test vyhÄ¾adÃ¡vania: '{query}'")
            results = self.collection.query(
                query_texts=[query],
                n_results=3
            )
            
            if results['documents'] and results['documents'][0]:
                for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
                    print(f"\n{i+1}. {meta.get('law_id', 'N/A')} - {meta.get('paragraph', 'N/A')}")
                    print(f"   {doc[:200]}...")
            else:
                print("âŒ Å½iadne vÃ½sledky")
                
        except Exception as e:
            print(f"âŒ Chyba pri teste: {e}")


def main():
    """HlavnÃ¡ funkcia"""
    print("ğŸš€ NaÄÃ­tavanie prÃ¡vnych textov do ChromaDB")
    print("=" * 50)
    
    # Vytvor loader
    loader = LegalTextLoader()
    
    # NaÄÃ­taj vÅ¡etky sÃºbory
    count = loader.load_all_files()
    
    if count > 0:
        print(f"\nâœ… ÃšspeÅ¡ne naÄÃ­tanÃ½ch {count} chunkov!")
        
        # Test vyhÄ¾adÃ¡vania
        loader.test_search("vlastnÃ­ctvo")
        loader.test_search("spoloÄnosÅ¥ s ruÄenÃ­m obmedzenÃ½m")
        
    else:
        print("\nâŒ Å½iadne dÃ¡ta neboli naÄÃ­tanÃ©")


if __name__ == "__main__":
    main()
