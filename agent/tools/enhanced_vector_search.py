"""
Roz≈°√≠ren√Ω vector search s fulltext mo≈ænos≈•ami
"""

from typing import List, Dict, Any, Optional, Type, Union
from langchain.tools import BaseTool
from pydantic import Field
import re

# Fallback pre ChromaDB ak nie je dostupn√©
try:
    import chromadb
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False


class EnhancedVectorSearchTool(BaseTool):
    """Roz≈°√≠ren√Ω vector search s podporou fulltext vyhƒæad√°vania"""
    
    name: str = "enhanced_vector_search"
    description: str = """
    Pokroƒçil√Ω vyhƒæad√°vac√≠ n√°stroj kombinuj√∫ci s√©mantick√© vyhƒæad√°vanie a fulltext search v datab√°ze slovensk√Ωch z√°konov.
    
    Podporovan√© form√°ty dotazov:
    1. S√©mantick√© vyhƒæad√°vanie: "povinnosti konateƒæa"
    2. Fulltext search: "contains:spoloƒçnos≈•"
    3. Regex vyhƒæad√°vanie: "regex:¬ß\\s*135[a-z]*"
    4. Kombinovan√©: "law:513/1991 contains:s√∫d" (hƒæad√° text "s√∫d" len v z√°kone 513/1991)
    5. Neg√°cia: "not_contains:fyzick√° osoba"
    
    Datab√°za obsahuje: 40/1964, 513/1991, 530/2003, 300/2005, 160/2015, 161/2015
    
    Input: vyhƒæad√°vac√≠ dotaz s voliteƒæn√Ωmi prefixmi (string)
    """
    
    # Pydantic fields
    collection_name: str = Field(default="legal_documents")
    client: Optional[Any] = Field(default=None, exclude=True)
    collection: Optional[Any] = Field(default=None, exclude=True)
    embedding_function: Optional[Any] = Field(default=None, exclude=True)
    
    def __init__(self, collection_name: str = "legal_documents", **kwargs):
        super().__init__(collection_name=collection_name, **kwargs)
        if CHROMADB_AVAILABLE:
            try:
                # Vytvor PRESNE ROVNAK√ù embedding model ako pri vytv√°ran√≠ datab√°zy
                try:
                    from sentence_transformers import SentenceTransformer
                    
                    # Vytvor embedding funkciu IDENTICK√ö s load_law_texts.py
                    class MultilingualEmbeddingFunction:
                        def __init__(self, model_name):
                            from sentence_transformers import SentenceTransformer
                            self.model = SentenceTransformer(model_name)
                            self.model_name = model_name
                            self.name = f"multilingual-{model_name}"
                        
                        def __call__(self, input):
                            import numpy as np
                            
                            # Z√≠skaj embeddings
                            embeddings = self.model.encode(input)
                            
                            # Normalizuj PRESNE ako v load_law_texts.py
                            if len(embeddings.shape) == 1:
                                # Jeden vektor
                                norm = np.linalg.norm(embeddings)
                                if norm > 0:
                                    embeddings = embeddings / norm
                            else:
                                # Viac vektorov
                                norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
                                embeddings = embeddings / np.maximum(norms, 1e-8)
                            
                            return embeddings.tolist()
                    
                    # Pou≈æ√≠vame PRESNE ROVNAK√ù model ako v datab√°ze
                    self.embedding_function = MultilingualEmbeddingFunction("paraphrase-multilingual-MiniLM-L12-v2")
                    
                    self.client = chromadb.PersistentClient(path="data/vector_db")
                    print("‚úÖ Enhanced Vector Search s multilingual embedding modelom")
                    
                except ImportError:
                    print("‚ùå Sentence transformers nie s√∫ dostupn√©")
                    self.client = None
                    self.embedding_function = None
                
                self._init_collection()
            except Exception as e:
                print(f"‚ùå Chyba pri inicializ√°cii Enhanced Vector Search: {e}")
                self.client = None
                self.collection = None
        else:
            self.client = None
            self.collection = None
    
    def _init_collection(self):
        """Inicializuje existuj√∫cu vector collection"""
        if not self.client:
            return
            
        try:
            collections = self.client.list_collections()
            collection_names = [col.name for col in collections]
            
            if self.collection_name not in collection_names:
                print(f"‚ùå Collection '{self.collection_name}' neexistuje")
                print(f"üìã Dostupn√© kolekcie: {collection_names}")
                self.collection = None
                return
            
            self.collection = self.client.get_collection(name=self.collection_name)
            count = self.collection.count()
            
            if count > 0:
                print(f"‚úÖ Enhanced search pripraven√Ω s {count} dokumentmi")
            else:
                print("‚ö†Ô∏è Collection je pr√°zdna")
                
        except Exception as e:
            print(f"‚ùå Chyba pri naƒç√≠tavan√≠ collection: {e}")
            self.collection = None
    
    def _parse_query(self, query: str) -> Dict[str, Any]:
        """Parsuje pokroƒçil√© query pr√≠kazy"""
        parsed = {
            'semantic_query': None,
            'where_filters': {},
            'where_document': {},
            'search_type': 'semantic'
        }
        
        # Rozpoznaj r√¥zne typy dotazov
        query = query.strip()
        
        # 1. Fulltext search patterns
        if query.startswith('contains:'):
            parsed['search_type'] = 'fulltext'
            text = query[9:]  # Odstr√°≈à 'contains:'
            parsed['where_document'] = {'$contains': text}
            return parsed
        
        if query.startswith('not_contains:'):
            parsed['search_type'] = 'fulltext'
            text = query[13:]  # Odstr√°≈à 'not_contains:'
            parsed['where_document'] = {'$not_contains': text}
            return parsed
        
        if query.startswith('regex:'):
            parsed['search_type'] = 'fulltext'
            pattern = query[6:]  # Odstr√°≈à 'regex:'
            parsed['where_document'] = {'$regex': pattern}
            return parsed
        
        # 2. Kombinovan√© queries (law:XXX contains:YYY)
        if ' ' in query and any(prefix in query for prefix in ['law:', 'contains:', 'regex:', 'not_contains:']):
            parts = query.split()
            for part in parts:
                if part.startswith('law:'):
                    parsed['where_filters']['law_id'] = part[4:]
                elif part.startswith('contains:'):
                    parsed['where_document']['$contains'] = part[9:]
                    parsed['search_type'] = 'combined'
                elif part.startswith('not_contains:'):
                    parsed['where_document']['$not_contains'] = part[13:]
                    parsed['search_type'] = 'combined'
                elif part.startswith('regex:'):
                    parsed['where_document']['$regex'] = part[6:]
                    parsed['search_type'] = 'combined'
                else:
                    # Zvy≈°n√© slov√° s√∫ pre s√©mantick√© vyhƒæad√°vanie
                    if parsed['semantic_query'] is None:
                        parsed['semantic_query'] = part
                    else:
                        parsed['semantic_query'] += ' ' + part
            
            return parsed
        
        # 3. ƒåist√Ω s√©mantick√Ω dotaz
        parsed['semantic_query'] = query
        return parsed
    
    def _fulltext_search(self, where_filters: Dict, where_document: Dict, limit: int = 5) -> List[Dict]:
        """Vykonaj fulltext search"""
        try:
            # Z√°kladn√Ω fulltext search
            kwargs = {'limit': limit, 'include': ['documents', 'metadatas']}
            
            if where_filters:
                kwargs['where'] = where_filters
            
            if where_document:
                kwargs['where_document'] = where_document
            
            results = self.collection.get(**kwargs)
            
            # Form√°tuj v√Ωsledky
            formatted = []
            for i, (doc, metadata) in enumerate(zip(results['documents'], results['metadatas'])):
                law_id = metadata.get('law_id', 'N/A')
                paragraph = metadata.get('paragraph', 'N/A')
                title = metadata.get('title', '')
                
                formatted.append({
                    'rank': i + 1,
                    'law_id': law_id,
                    'paragraph': paragraph,
                    'title': title,
                    'text': doc,
                    'similarity': 'Fulltext match',
                    'search_type': 'fulltext'
                })
            
            return formatted
            
        except Exception as e:
            print(f"Chyba pri fulltext search: {e}")
            return []
    
    def _semantic_search(self, query: str, where_filters: Dict = None, limit: int = 5) -> List[Dict]:
        """Vykonaj s√©mantick√© vyhƒæad√°vanie"""
        try:
            if not self.embedding_function:
                return []
            
            # Vytvor embedding pre query
            query_embedding = self.embedding_function([query])
            
            # Query parameters
            kwargs = {
                'query_embeddings': query_embedding,
                'n_results': limit,
                'include': ['documents', 'metadatas', 'distances']
            }
            
            if where_filters:
                kwargs['where'] = where_filters
            
            results = self.collection.query(**kwargs)
            
            # Form√°tuj v√Ωsledky
            formatted = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )):
                similarity = round((1 - distance) * 100, 1)
                law_id = metadata.get('law_id', 'N/A')
                paragraph = metadata.get('paragraph', 'N/A')
                title = metadata.get('title', '')
                
                formatted.append({
                    'rank': i + 1,
                    'law_id': law_id,
                    'paragraph': paragraph,
                    'title': title,
                    'text': doc,
                    'similarity': f"{similarity}%",
                    'search_type': 'semantic'
                })
            
            return formatted
            
        except Exception as e:
            print(f"Chyba pri semantic search: {e}")
            return []
    
    def _combined_search(self, parsed_query: Dict, limit: int = 5) -> List[Dict]:
        """Kombinuje s√©mantick√© a fulltext vyhƒæad√°vanie"""
        results = []
        
        # Ak m√°me s√©mantick√Ω dotaz, pridaj s√©mantick√© v√Ωsledky
        if parsed_query['semantic_query']:
            semantic_results = self._semantic_search(
                parsed_query['semantic_query'],
                parsed_query['where_filters'],
                limit // 2 + 1
            )
            results.extend(semantic_results)
        
        # Pridaj fulltext v√Ωsledky
        if parsed_query['where_document']:
            fulltext_results = self._fulltext_search(
                parsed_query['where_filters'],
                parsed_query['where_document'],
                limit // 2 + 1
            )
            results.extend(fulltext_results)
        
        # Odstr√°≈à duplik√°ty a obmedzi na limit
        seen_ids = set()
        unique_results = []
        for result in results:
            result_id = f"{result['law_id']}_{result['paragraph']}"
            if result_id not in seen_ids:
                seen_ids.add(result_id)
                unique_results.append(result)
                if len(unique_results) >= limit:
                    break
        
        return unique_results
    
    def _format_results(self, results: List[Dict]) -> str:
        """Form√°tuje v√Ωsledky do ƒçitateƒæn√©ho form√°tu"""
        if not results:
            return "Nena≈°li sa ≈æiadne relevantn√© dokumenty pre zadan√Ω dotaz."
        
        formatted_results = []
        for result in results:
            formatted_results.append(
                f"**V√Ωsledok {result['rank']}** (podobnos≈•: {result['similarity']}, typ: {result['search_type']})\n"
                f"Z√°kon: {result['law_id']} - {result['paragraph']}\n"
                f"N√°zov: {result['title']}\n"
                f"Text: {result['text'][:300]}{'...' if len(result['text']) > 300 else ''}\n"
            )
        
        return "\n".join(formatted_results)
    
    def _run(self, query: str) -> str:
        """Hlavn√° vyhƒæad√°vacia funkcia"""
        if not self.collection:
            return f"Enhanced vector search nie je dostupn√Ω - kolekcia '{self.collection_name}' neexistuje."
        
        try:
            # Parsuj dotaz
            parsed_query = self._parse_query(query)
            print(f"üîç Parsed query: {parsed_query}")  # Debug info
            
            # Vykonaj vyhƒæad√°vanie podƒæa typu
            if parsed_query['search_type'] == 'semantic':
                results = self._semantic_search(parsed_query['semantic_query'])
            elif parsed_query['search_type'] == 'fulltext':
                results = self._fulltext_search(
                    parsed_query['where_filters'],
                    parsed_query['where_document']
                )
            elif parsed_query['search_type'] == 'combined':
                results = self._combined_search(parsed_query)
            else:
                return "Nerozoznan√Ω typ dotazu."
            
            return self._format_results(results)
            
        except Exception as e:
            return f"Chyba pri enhanced vector search: {str(e)}"
    
    async def _arun(self, query: str) -> str:
        """Async verzia"""
        return self._run(query)


def get_enhanced_search_tool():
    """Vr√°ti enhanced vector search tool"""
    return EnhancedVectorSearchTool()
